{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Anne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Anne\\AppData\\Local\\Temp\\ipykernel_20212\\3013541413.py:22: FutureWarning: Parsed string \"Mon Apr 06 22:19:45 PDT 2009\" included an un-recognized timezone \"PDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df = pd.read_csv(DATA_NAME,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                date      flag             user  \\\n",
       "0          0  1467810369 2009-04-06 22:19:45  NO_QUERY  _TheSpecialOne_   \n",
       "1          0  1467810672 2009-04-06 22:19:49  NO_QUERY    scotthamilton   \n",
       "2          0  1467810917 2009-04-06 22:19:53  NO_QUERY         mattycus   \n",
       "3          0  1467811184 2009-04-06 22:19:57  NO_QUERY          ElleCTF   \n",
       "4          0  1467811193 2009-04-06 22:19:57  NO_QUERY           Karoli   \n",
       "\n",
       "                                                                                                                 tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "2                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "3                                                                      my whole body feels itchy and like its on fire   \n",
       "4      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "\n",
    "DATA_NAME = 'sentiment140/training.1600000.processed.noemoticon.csv'\n",
    "ENCODING = 'latin-1'\n",
    "COLUMN_NAMES = ['sentiment', 'id', 'date', 'flag', 'user', 'tweet']\n",
    "NROWS = 1600000\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_NAME,\n",
    "                 encoding=ENCODING,\n",
    "                 header=None,\n",
    "                 names=COLUMN_NAMES,\n",
    "                 nrows=NROWS,\n",
    "                 parse_dates=['date'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(stop_words, key=str.casefold)\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_multiple_letters(tweet):\n",
    "  reduced_tokens = []\n",
    "  wrapper = [False]\n",
    "\n",
    "  def replace(hit):\n",
    "      if not wrapper[0]:\n",
    "        wrapper[0] = True\n",
    "        return hit[0]\n",
    "      else:\n",
    "        return \"\"\n",
    "\n",
    "  for word in tweet.split():\n",
    "      reduced_tokens.append(re.sub(r\"(.)(?=\\1+)\", replace, word))\n",
    "      wrapper[0] = False\n",
    "\n",
    "  return ' '.join(reduced_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_recoding_map = {\n",
    "        0: 0,\n",
    "        4: 1\n",
    "    }\n",
    "\n",
    "def sentiment_recoding(sentiment):\n",
    "  return sentiment_recoding_map[int(sentiment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# csrt with apostrophe\n",
    "def lem_tweets_with_apostrophe(tweet):\n",
    "  # clean tweets from mentions, hashtags, URLs, special characters except \"'\"\n",
    "  tweet = re.sub(r'@\\w+|#\\w+|http\\S+|[^A-Za-z0-9\\']+', ' ', tweet).lower().strip()\n",
    "\n",
    "  # shorten multiple letters\n",
    "  tweet = shorten_multiple_letters(tweet)\n",
    "\n",
    "  # remove stopwords\n",
    "  tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "\n",
    "  # tokenize\n",
    "  tweet = nltk.word_tokenize(tweet)\n",
    "\n",
    "  # lemmatize\n",
    "  tweet = [lemmatizer.lemmatize(word) for word in tweet]\n",
    "\n",
    "  return ' '.join(tweet)\n",
    "\n",
    "\n",
    "# cstr no apostrophe\n",
    "def lem_tweets_no_apostrophe(tweet):\n",
    "  # clean tweets from mentions, hashtags, URLs, special characters except \"'\"\n",
    "  tweet = re.sub(r'@\\w+|#\\w+|http\\S+|[^A-Za-z0-9]+', ' ', tweet).lower().strip()\n",
    "\n",
    "  # shorten multiple letters\n",
    "  tweet = shorten_multiple_letters(tweet)\n",
    "\n",
    "  # tokenize\n",
    "  tweet = nltk.word_tokenize(tweet)\n",
    "\n",
    "  # remove stopwords\n",
    "  tweet = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "  # lemmatize\n",
    "  tweet = [lemmatizer.lemmatize(word) for word in tweet]\n",
    "\n",
    "  return ' '.join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# csrt with apostrophe\n",
    "def stem_tweets_with_apostrophe(tweet):\n",
    "  # clean tweets from mentions, hashtags, URLs, special characters except \"'\"\n",
    "  tweet = re.sub(r'@\\w+|#\\w+|http\\S+|[^A-Za-z0-9\\']+', ' ', tweet).lower().strip()\n",
    "\n",
    "  # shorten multiple letters\n",
    "  tweet = shorten_multiple_letters(tweet)\n",
    "\n",
    "  # remove stopwords\n",
    "  tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "\n",
    "  # tokenize\n",
    "  tweet = nltk.word_tokenize(tweet)\n",
    "\n",
    "  # lemmatize\n",
    "  tweet = [stemmer.stem(word) for word in tweet]\n",
    "\n",
    "  return ' '.join(tweet)\n",
    "\n",
    "\n",
    "# cstr no apostrophe\n",
    "def stem_tweets_no_apostrophe(tweet):\n",
    "  # clean tweets from mentions, hashtags, URLs, special characters except \"'\"\n",
    "  tweet = re.sub(r'@\\w+|#\\w+|http\\S+|[^A-Za-z0-9]+', ' ', tweet).lower().strip()\n",
    "\n",
    "  # shorten multiple letters\n",
    "  tweet = shorten_multiple_letters(tweet)\n",
    "\n",
    "  # tokenize\n",
    "  tweet = nltk.word_tokenize(tweet)\n",
    "\n",
    "  # remove stopwords\n",
    "  tweet = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "  # lemmatize\n",
    "  tweet = [stemmer.stem(word) for word in tweet]\n",
    "\n",
    "  return ' '.join(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns\n",
    "df.drop(columns=['id', 'date', 'flag', 'user'], inplace=True)\n",
    "\n",
    "# sentement recoding\n",
    "df.sentiment = df.sentiment.apply(lambda sentiment: sentiment_recoding(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating different preprocessed datasets by applying preprocessing steps\n",
    "#### Lemmatized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment  \\\n",
      "0          0   \n",
      "1          0   \n",
      "2          0   \n",
      "3          0   \n",
      "4          0   \n",
      "\n",
      "                                                                          tweet  \n",
      "0                           aww that 's bummer shoulda got david carr third day  \n",
      "1  upset ca n't update facebook texting might cry result school today also blah  \n",
      "2                            dived many time ball managed save 50 rest go bound  \n",
      "3                                               whole body feel itchy like fire  \n",
      "4                                                  behaving i 'm mad ca n't see  \n",
      "with  (1592606, 2)\n",
      "   sentiment  \\\n",
      "0          0   \n",
      "1          0   \n",
      "2          0   \n",
      "3          0   \n",
      "4          0   \n",
      "\n",
      "                                                                   tweet  \n",
      "0                            aww bummer shoulda got david carr third day  \n",
      "1  upset update facebook texting might cry result school today also blah  \n",
      "2                     dived many time ball managed save 50 rest go bound  \n",
      "3                                        whole body feel itchy like fire  \n",
      "4                                                       behaving mad see  \n",
      "no  (1592203, 2)\n"
     ]
    }
   ],
   "source": [
    "df_with_apostrophe_lem = df.copy()\n",
    "df_no_apostrophe_lem = df.copy()\n",
    "\n",
    "\n",
    "# 1) preprocessing with lemmatizing and with apostrophe\n",
    "df_with_apostrophe_lem.tweet = df_with_apostrophe_lem.tweet.apply(lambda tweet: lem_tweets_with_apostrophe(tweet))\n",
    "\n",
    "# removing empty rows\n",
    "df_with_apostrophe_lem.drop(df_with_apostrophe_lem[df_with_apostrophe_lem.tweet == ''].index, inplace=True)\n",
    "#df_with_apostrophe_lem.tweet = df_with_apostrophe_lem.tweet.apply(lambda tweet: tweet.split())\n",
    "\n",
    "print(df_with_apostrophe_lem.head(5))\n",
    "print('with ', df_with_apostrophe_lem.shape)\n",
    "\n",
    "\n",
    "# 2) preprocessing with lemmatizing and no apostrophe\n",
    "df_no_apostrophe_lem.tweet = df_no_apostrophe_lem.tweet.apply(lambda tweet: lem_tweets_no_apostrophe(tweet))\n",
    "\n",
    "# removing empty rows\n",
    "df_no_apostrophe_lem.drop(df_no_apostrophe_lem[df_no_apostrophe_lem.tweet == ''].index, inplace=True)\n",
    "#df_no_apostrophe_lem.tweet = df_no_apostrophe_lem.tweet.apply(lambda tweet: tweet.split())\n",
    "\n",
    "print(df_no_apostrophe_lem.head(5))\n",
    "print('no ', df_no_apostrophe_lem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df_with_apostrophe_lem.to_csv('sentiment140/sentiment140_lem_all_with_apostrophe.csv', index=False, header=False)\n",
    "\n",
    "df_no_apostrophe_lem.to_csv('sentiment140/sentiment140_lem_all_no_apostrophe.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment  \\\n",
      "0          0   \n",
      "1          0   \n",
      "2          0   \n",
      "3          0   \n",
      "4          0   \n",
      "\n",
      "                                                                      tweet  \n",
      "0                       aww that 's bummer shoulda got david carr third day  \n",
      "1  upset ca n't updat facebook text might cri result school today also blah  \n",
      "2                           dive mani time ball manag save 50 rest go bound  \n",
      "3                                           whole bodi feel itchi like fire  \n",
      "4                                                 behav i 'm mad ca n't see  \n",
      "with  (1592606, 2)\n",
      "   sentiment  \\\n",
      "0          0   \n",
      "1          0   \n",
      "2          0   \n",
      "3          0   \n",
      "4          0   \n",
      "\n",
      "                                                               tweet  \n",
      "0                        aww bummer shoulda got david carr third day  \n",
      "1  upset updat facebook text might cri result school today also blah  \n",
      "2                    dive mani time ball manag save 50 rest go bound  \n",
      "3                                    whole bodi feel itchi like fire  \n",
      "4                                                      behav mad see  \n",
      "no  (1592203, 2)\n"
     ]
    }
   ],
   "source": [
    "df_with_apostrophe_stem = df.copy()\n",
    "df_no_apostrophe_stem = df.copy()\n",
    "\n",
    "\n",
    "# 1) preprocessing with stemming and with apostrophe\n",
    "df_with_apostrophe_stem.tweet = df_with_apostrophe_stem.tweet.apply(lambda tweet: stem_tweets_with_apostrophe(tweet))\n",
    "\n",
    "# removing empty rows\n",
    "df_with_apostrophe_stem.drop(df_with_apostrophe_stem[df_with_apostrophe_stem.tweet == ''].index, inplace=True)\n",
    "#df_with_apostrophe_stem.tweet = df_with_apostrophe_stem.tweet.apply(lambda tweet: tweet.split())\n",
    "\n",
    "print(df_with_apostrophe_stem.head(5))\n",
    "print('with ', df_with_apostrophe_stem.shape)\n",
    "\n",
    "\n",
    "# 2) preprocessing with stemming and no apostrophe\n",
    "df_no_apostrophe_stem.tweet = df_no_apostrophe_stem.tweet.apply(lambda tweet: stem_tweets_no_apostrophe(tweet))\n",
    "\n",
    "# removing empty rows\n",
    "df_no_apostrophe_stem.drop(df_no_apostrophe_stem[df_no_apostrophe_stem.tweet == ''].index, inplace=True)\n",
    "#df_no_apostrophe_stem.tweet = df_no_apostrophe_stem.tweet.apply(lambda tweet: tweet.split())\n",
    "\n",
    "print(df_no_apostrophe_stem.head(5))\n",
    "print('no ', df_no_apostrophe_stem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df_with_apostrophe_stem.to_csv('sentiment140/sentiment140_stem_all_with_apostrophe.csv', index=False, header=False)\n",
    "\n",
    "df_no_apostrophe_stem.to_csv('sentiment140/sentiment140_stem_all_no_apostrophe.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data pre-processing",
   "id": "579904f852a65b0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "imports",
   "id": "3ead0d157c01359c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-30T09:09:03.102441Z",
     "start_time": "2024-11-30T09:09:01.432176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import pytz\n",
    "import dateutil.parser\n",
    "import urllib.request\n",
    "import zipfile\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants",
   "id": "e8ff3557b014606b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:09:04.701680Z",
     "start_time": "2024-11-30T09:09:04.571086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATASET\n",
    "DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING_ISO = \"ISO-8859-1\"\n",
    "DATASET_ENCODING_UTF = \"UTF-8\"\n",
    "\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9']+\"\n",
    "\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "\n",
    "# TIMEZONE\n",
    "TZINFOS = { 'PDT': pytz.timezone('US/Pacific')}\n",
    "\n",
    "# FILE\n",
    "URL = 'https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
    "TEST_FILE_NAME = \"testdata.manual.2009.06.14.csv\"\n",
    "TRAIN_FILE_NAME = \"training.1600000.processed.noemoticon.csv\""
   ],
   "id": "95c5bb9b5e368fb9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load file",
   "id": "dba55f5a42300382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:13:47.447927Z",
     "start_time": "2024-11-20T17:13:36.472039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filehandle, _ = urllib.request.urlretrieve(URL)\n",
    "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
    "train_file = zip_file_object.open(TRAIN_FILE_NAME)\n",
    "test_file = zip_file_object.open(TEST_FILE_NAME)\n",
    "f = open(TRAIN_FILE_NAME, \"wb\")\n",
    "f.write(train_file.read())\n",
    "f = open(TEST_FILE_NAME, \"wb\")\n",
    "f.write(test_file.read())"
   ],
   "id": "bf21bd366cae5f68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74326"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load Dataframe",
   "id": "13aea151acc64614"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:11:53.644824Z",
     "start_time": "2024-11-30T09:09:13.333522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(TRAIN_FILE_NAME, names=DATASET_COLUMNS, encoding=DATASET_ENCODING_ISO, header=None, converters={'date': lambda date: dateutil.parser.parse(date, tzinfos=TZINFOS)})\n",
    "df_test = pd.read_csv(TEST_FILE_NAME, names=DATASET_COLUMNS, encoding=DATASET_ENCODING_ISO, header=None, converters={'date': lambda date: dateutil.parser.parse(date, tzinfos=TZINFOS)})"
   ],
   "id": "f1be50dd5d3062fd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "print infos",
   "id": "d726aea2a1699cb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:17:07.153614Z",
     "start_time": "2024-11-20T17:17:06.949427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.head(5))\n",
    "print(df.dtypes)\n",
    "print(df.info())    "
   ],
   "id": "98dd0fde223f6f46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         ids                      date      flag             user  \\\n",
      "0          0  1467810369 2009-04-06 23:12:45-07:00  NO_QUERY  _TheSpecialOne_   \n",
      "1          0  1467810672 2009-04-06 23:12:49-07:00  NO_QUERY    scotthamilton   \n",
      "2          0  1467810917 2009-04-06 23:12:53-07:00  NO_QUERY         mattycus   \n",
      "3          0  1467811184 2009-04-06 23:12:57-07:00  NO_QUERY          ElleCTF   \n",
      "4          0  1467811193 2009-04-06 23:12:57-07:00  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n",
      "sentiment                         int64\n",
      "ids                               int64\n",
      "date         datetime64[ns, US/Pacific]\n",
      "flag                             object\n",
      "user                             object\n",
      "text                             object\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype                     \n",
      "---  ------     --------------    -----                     \n",
      " 0   sentiment  1600000 non-null  int64                     \n",
      " 1   ids        1600000 non-null  int64                     \n",
      " 2   date       1600000 non-null  datetime64[ns, US/Pacific]\n",
      " 3   flag       1600000 non-null  object                    \n",
      " 4   user       1600000 non-null  object                    \n",
      " 5   text       1600000 non-null  object                    \n",
      "dtypes: datetime64[ns, US/Pacific](1), int64(2), object(3)\n",
      "memory usage: 73.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load stopwords",
   "id": "30401fddf4a668eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:17:08.848328Z",
     "start_time": "2024-11-20T17:17:08.838069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_test.head(5))\n",
    "print(df_test.dtypes)\n",
    "print(df_test.info())"
   ],
   "id": "4861760265a73fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment  ids                      date     flag      user  \\\n",
      "0          4    3 2009-05-11 03:17:40+00:00  kindle2    tpryan   \n",
      "1          4    4 2009-05-11 03:18:03+00:00  kindle2    vcu451   \n",
      "2          4    5 2009-05-11 03:18:54+00:00  kindle2    chadfu   \n",
      "3          4    6 2009-05-11 03:19:04+00:00  kindle2     SIX15   \n",
      "4          4    7 2009-05-11 03:21:41+00:00  kindle2  yamarama   \n",
      "\n",
      "                                                text  \n",
      "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
      "1  Reading my kindle2...  Love it... Lee childs i...  \n",
      "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
      "3  @kenburbary You'll love your Kindle2. I've had...  \n",
      "4  @mikefish  Fair enough. But i have the Kindle2...  \n",
      "sentiment                      int64\n",
      "ids                            int64\n",
      "date         datetime64[ns, tzutc()]\n",
      "flag                          object\n",
      "user                          object\n",
      "text                          object\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype                  \n",
      "---  ------     --------------  -----                  \n",
      " 0   sentiment  498 non-null    int64                  \n",
      " 1   ids        498 non-null    int64                  \n",
      " 2   date       498 non-null    datetime64[ns, tzutc()]\n",
      " 3   flag       498 non-null    object                 \n",
      " 4   user       498 non-null    object                 \n",
      " 5   text       498 non-null    object                 \n",
      "dtypes: datetime64[ns, tzutc()](1), int64(2), object(3)\n",
      "memory usage: 23.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:12:05.782589Z",
     "start_time": "2024-11-30T09:12:05.501413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ],
   "id": "b7ba91386b7871c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "map sentiment to constants",
   "id": "8ecbda08390c15d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:12:07.192036Z",
     "start_time": "2024-11-30T09:12:07.187412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]"
   ],
   "id": "3e43077ea2a1df96",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "stemming and removing stopwords and letters which occur more than 2 times consecutive",
   "id": "687fbbbe7ea134a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:12:10.228145Z",
     "start_time": "2024-11-30T09:12:10.222559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_text(text, remove_stopwords=True):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = text.split()\n",
    "    reduced_tokens = []\n",
    "    wrapper = [False]\n",
    "    def replace(hit):\n",
    "        if not wrapper[0]:\n",
    "            wrapper[0] = True\n",
    "            return hit[0]\n",
    "        else:\n",
    "            return \"\"\n",
    "    for word in tokens:\n",
    "        if not remove_stopwords or word not in stop_words:\n",
    "            reduced_tokens.append(re.sub(r\"(.)(?=\\1+)\", replace, word))\n",
    "        wrapper[0] = False\n",
    "    return reduced_tokens"
   ],
   "id": "466a59c9681778e7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:12:12.502666Z",
     "start_time": "2024-11-30T09:12:12.498919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stem_text(tokens):\n",
    "    return ' '.join([stemmer.stem(word) for word in tokens])"
   ],
   "id": "a86854822bd1190a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:12:13.471926Z",
     "start_time": "2024-11-30T09:12:13.468491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize_text(tokens):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])"
   ],
   "id": "260f79a164640967",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "preprocessing",
   "id": "38da764f58cacf7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:17:56.704323Z",
     "start_time": "2024-11-30T09:12:14.777865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['mapped_sentiment'] = df.sentiment.apply(decode_sentiment)\n",
    "df['tokenized_text'] = df.text.apply(tokenize_text)\n",
    "df['tokenized_text_with_stopwords'] = df.text.apply(lambda tweet:tokenize_text(tweet, False))\n",
    "df['stemmed_text'] = df.tokenized_text.apply(stem_text)\n",
    "df['lemmatized_text'] = df.tokenized_text.apply(lemmatize_text)\n",
    "df['lemmatized_text_with_stopwords'] = df.tokenized_text_with_stopwords.apply(lemmatize_text)\n"
   ],
   "id": "74bf3aad3a2cc006",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:17:56.888807Z",
     "start_time": "2024-11-30T09:17:56.746242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test['mapped_sentiment'] = df_test.sentiment.apply(decode_sentiment)\n",
    "df_test['tokenized_text'] = df_test.text.apply(tokenize_text)\n",
    "df_test['tokenized_text_with_stopwords'] = df_test.text.apply(lambda tweet:tokenize_text(tweet, False))\n",
    "df_test['stemmed_text'] = df_test.tokenized_text.apply(stem_text)\n",
    "df_test['lemmatized_text'] = df_test.tokenized_text.apply(lemmatize_text)\n",
    "df_test['lemmatized_text_with_stopwords'] = df_test.tokenized_text_with_stopwords.apply(lemmatize_text)"
   ],
   "id": "5c71c30c5e5fc687",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:17:57.502309Z",
     "start_time": "2024-11-30T09:17:56.895313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.head(5))\n",
    "df.info()"
   ],
   "id": "3f69b457da83ea26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         ids                      date      flag             user  \\\n",
      "0          0  1467810369 2009-04-06 23:12:45-07:00  NO_QUERY  _TheSpecialOne_   \n",
      "1          0  1467810672 2009-04-06 23:12:49-07:00  NO_QUERY    scotthamilton   \n",
      "2          0  1467810917 2009-04-06 23:12:53-07:00  NO_QUERY         mattycus   \n",
      "3          0  1467811184 2009-04-06 23:12:57-07:00  NO_QUERY          ElleCTF   \n",
      "4          0  1467811193 2009-04-06 23:12:57-07:00  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text mapped_sentiment  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...         NEGATIVE   \n",
      "1  is upset that he can't update his Facebook by ...         NEGATIVE   \n",
      "2  @Kenichan I dived many times for the ball. Man...         NEGATIVE   \n",
      "3    my whole body feels itchy and like its on fire          NEGATIVE   \n",
      "4  @nationwideclass no, it's not behaving at all....         NEGATIVE   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [aww, that's, bummer, shoulda, got, david, car...   \n",
      "1  [upset, can't, update, facebook, texting, migh...   \n",
      "2  [dived, many, times, ball, managed, save, 50, ...   \n",
      "3            [whole, body, feels, itchy, like, fire]   \n",
      "4                   [behaving, i'm, mad, can't, see]   \n",
      "\n",
      "                       tokenized_text_with_stopwords  \\\n",
      "0  [aww, that's, a, bummer, you, shoulda, got, da...   \n",
      "1  [is, upset, that, he, can't, update, his, face...   \n",
      "2  [i, dived, many, times, for, the, ball, manage...   \n",
      "3  [my, whole, body, feels, itchy, and, like, its...   \n",
      "4  [no, it's, not, behaving, at, all, i'm, mad, w...   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0   aww that bummer shoulda got david carr third day   \n",
      "1  upset can't updat facebook text might cri resu...   \n",
      "2    dive mani time ball manag save 50 rest go bound   \n",
      "3                    whole bodi feel itchi like fire   \n",
      "4                            behav i'm mad can't see   \n",
      "\n",
      "                                     lemmatized_text  \\\n",
      "0  aww that's bummer shoulda got david carr third...   \n",
      "1  upset can't update facebook texting might cry ...   \n",
      "2  dived many time ball managed save 50 rest go b...   \n",
      "3                    whole body feel itchy like fire   \n",
      "4                         behaving i'm mad can't see   \n",
      "\n",
      "                      lemmatized_text_with_stopwords  \n",
      "0  aww that's a bummer you shoulda got david carr...  \n",
      "1  is upset that he can't update his facebook by ...  \n",
      "2  i dived many time for the ball managed to save...  \n",
      "3       my whole body feel itchy and like it on fire  \n",
      "4  no it's not behaving at all i'm mad why am i h...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                          Non-Null Count    Dtype                     \n",
      "---  ------                          --------------    -----                     \n",
      " 0   sentiment                       1600000 non-null  int64                     \n",
      " 1   ids                             1600000 non-null  int64                     \n",
      " 2   date                            1600000 non-null  datetime64[ns, US/Pacific]\n",
      " 3   flag                            1600000 non-null  object                    \n",
      " 4   user                            1600000 non-null  object                    \n",
      " 5   text                            1600000 non-null  object                    \n",
      " 6   mapped_sentiment                1600000 non-null  object                    \n",
      " 7   tokenized_text                  1600000 non-null  object                    \n",
      " 8   tokenized_text_with_stopwords   1600000 non-null  object                    \n",
      " 9   stemmed_text                    1600000 non-null  object                    \n",
      " 10  lemmatized_text                 1600000 non-null  object                    \n",
      " 11  lemmatized_text_with_stopwords  1600000 non-null  object                    \n",
      "dtypes: datetime64[ns, US/Pacific](1), int64(2), object(9)\n",
      "memory usage: 146.5+ MB\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "write data to file",
   "id": "89385c0cd04c5fa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:17:57.529708Z",
     "start_time": "2024-11-30T09:17:57.517299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_test.head(5))\n",
    "df_test.info()"
   ],
   "id": "6acab6c3e972a77b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment  ids                      date     flag      user  \\\n",
      "0          4    3 2009-05-11 03:17:40+00:00  kindle2    tpryan   \n",
      "1          4    4 2009-05-11 03:18:03+00:00  kindle2    vcu451   \n",
      "2          4    5 2009-05-11 03:18:54+00:00  kindle2    chadfu   \n",
      "3          4    6 2009-05-11 03:19:04+00:00  kindle2     SIX15   \n",
      "4          4    7 2009-05-11 03:21:41+00:00  kindle2  yamarama   \n",
      "\n",
      "                                                text mapped_sentiment  \\\n",
      "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...         POSITIVE   \n",
      "1  Reading my kindle2...  Love it... Lee childs i...         POSITIVE   \n",
      "2  Ok, first assesment of the #kindle2 ...it fuck...         POSITIVE   \n",
      "3  @kenburbary You'll love your Kindle2. I've had...         POSITIVE   \n",
      "4  @mikefish  Fair enough. But i have the Kindle2...         POSITIVE   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0    [loove, kindle2, dx, cool, 2, fantastic, right]   \n",
      "1  [reading, kindle2, love, lee, childs, good, read]   \n",
      "2    [ok, first, assesment, kindle2, fucking, rocks]   \n",
      "3  [love, kindle2, i've, mine, months, never, loo...   \n",
      "4            [fair, enough, kindle2, think, perfect]   \n",
      "\n",
      "                       tokenized_text_with_stopwords  \\\n",
      "0  [i, loove, my, kindle2, not, that, the, dx, is...   \n",
      "1  [reading, my, kindle2, love, it, lee, childs, ...   \n",
      "2  [ok, first, assesment, of, the, kindle2, it, f...   \n",
      "3  [you'll, love, your, kindle2, i've, had, mine,...   \n",
      "4  [fair, enough, but, i, have, the, kindle2, and...   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0               loov kindle2 dx cool 2 fantast right   \n",
      "1              read kindle2 love lee child good read   \n",
      "2                   ok first asses kindle2 fuck rock   \n",
      "3  love kindle2 i'v mine month never look back ne...   \n",
      "4                  fair enough kindle2 think perfect   \n",
      "\n",
      "                                     lemmatized_text  \\\n",
      "0            loove kindle2 dx cool 2 fantastic right   \n",
      "1           reading kindle2 love lee child good read   \n",
      "2            ok first assesment kindle2 fucking rock   \n",
      "3  love kindle2 i've mine month never looked back...   \n",
      "4                  fair enough kindle2 think perfect   \n",
      "\n",
      "                      lemmatized_text_with_stopwords  \n",
      "0  i loove my kindle2 not that the dx is cool but...  \n",
      "1  reading my kindle2 love it lee child is good read  \n",
      "2  ok first assesment of the kindle2 it fucking rock  \n",
      "3  you'll love your kindle2 i've had mine for a f...  \n",
      "4  fair enough but i have the kindle2 and i think...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 12 columns):\n",
      " #   Column                          Non-Null Count  Dtype                  \n",
      "---  ------                          --------------  -----                  \n",
      " 0   sentiment                       498 non-null    int64                  \n",
      " 1   ids                             498 non-null    int64                  \n",
      " 2   date                            498 non-null    datetime64[ns, tzutc()]\n",
      " 3   flag                            498 non-null    object                 \n",
      " 4   user                            498 non-null    object                 \n",
      " 5   text                            498 non-null    object                 \n",
      " 6   mapped_sentiment                498 non-null    object                 \n",
      " 7   tokenized_text                  498 non-null    object                 \n",
      " 8   tokenized_text_with_stopwords   498 non-null    object                 \n",
      " 9   stemmed_text                    498 non-null    object                 \n",
      " 10  lemmatized_text                 498 non-null    object                 \n",
      " 11  lemmatized_text_with_stopwords  498 non-null    object                 \n",
      "dtypes: datetime64[ns, tzutc()](1), int64(2), object(9)\n",
      "memory usage: 46.8+ KB\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:18:30.716997Z",
     "start_time": "2024-11-30T09:17:57.544842Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv(\"train_data_prepared.csv\", encoding=DATASET_ENCODING_UTF, index=False)",
   "id": "5f13ae08d6e4b6c4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:18:30.747664Z",
     "start_time": "2024-11-30T09:18:30.730752Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.to_csv(\"test_data_prepared.csv\", encoding=DATASET_ENCODING_UTF, index=False)",
   "id": "c2ab73ae27052a5",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

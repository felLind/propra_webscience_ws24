{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data pre-processing",
   "id": "579904f852a65b0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "imports",
   "id": "3ead0d157c01359c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-17T17:41:24.964308Z",
     "start_time": "2024-11-17T17:41:23.546698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import pytz\n",
    "import dateutil.parser\n",
    "import urllib.request\n",
    "import zipfile\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants",
   "id": "e8ff3557b014606b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:42:19.034902Z",
     "start_time": "2024-11-17T17:42:18.933185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATASET\n",
    "DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "\n",
    "# TIMEZONE\n",
    "TZINFOS = { 'PDT': pytz.timezone('US/Pacific')}\n",
    "\n",
    "# FILE\n",
    "URL = 'https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
    "TEST_FILE_NAME = \"testdata.manual.2009.06.14.csv\"\n",
    "TRAIN_FILE_NAME = \"training.1600000.processed.noemoticon.csv\""
   ],
   "id": "95c5bb9b5e368fb9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load file",
   "id": "dba55f5a42300382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:42:39.958740Z",
     "start_time": "2024-11-17T17:42:21.685258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filehandle, _ = urllib.request.urlretrieve(URL)\n",
    "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
    "train_file = zip_file_object.open(TRAIN_FILE_NAME)\n",
    "f = open(TRAIN_FILE_NAME, \"wb\")\n",
    "f.write(train_file.read())"
   ],
   "id": "bf21bd366cae5f68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238803811"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load Dataframe",
   "id": "13aea151acc64614"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:45:27.406638Z",
     "start_time": "2024-11-17T17:42:49.492771Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(TRAIN_FILE_NAME, names=DATASET_COLUMNS, encoding=DATASET_ENCODING, header=None, converters={'date': lambda date: dateutil.parser.parse(date, tzinfos=TZINFOS)})",
   "id": "f1be50dd5d3062fd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "print infos",
   "id": "d726aea2a1699cb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:45:44.833495Z",
     "start_time": "2024-11-17T17:45:44.660448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.head(5))\n",
    "print(df.dtypes)\n",
    "print(df.info())    "
   ],
   "id": "98dd0fde223f6f46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         ids                      date      flag             user  \\\n",
      "0          0  1467810369 2009-04-06 23:12:45-07:00  NO_QUERY  _TheSpecialOne_   \n",
      "1          0  1467810672 2009-04-06 23:12:49-07:00  NO_QUERY    scotthamilton   \n",
      "2          0  1467810917 2009-04-06 23:12:53-07:00  NO_QUERY         mattycus   \n",
      "3          0  1467811184 2009-04-06 23:12:57-07:00  NO_QUERY          ElleCTF   \n",
      "4          0  1467811193 2009-04-06 23:12:57-07:00  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n",
      "sentiment                         int64\n",
      "ids                               int64\n",
      "date         datetime64[ns, US/Pacific]\n",
      "flag                             object\n",
      "user                             object\n",
      "text                             object\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype                     \n",
      "---  ------     --------------    -----                     \n",
      " 0   sentiment  1600000 non-null  int64                     \n",
      " 1   ids        1600000 non-null  int64                     \n",
      " 2   date       1600000 non-null  datetime64[ns, US/Pacific]\n",
      " 3   flag       1600000 non-null  object                    \n",
      " 4   user       1600000 non-null  object                    \n",
      " 5   text       1600000 non-null  object                    \n",
      "dtypes: datetime64[ns, US/Pacific](1), int64(2), object(3)\n",
      "memory usage: 73.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load stopwords",
   "id": "30401fddf4a668eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:45:49.887514Z",
     "start_time": "2024-11-17T17:45:49.543093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ],
   "id": "b7ba91386b7871c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "map sentiment to constants",
   "id": "8ecbda08390c15d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:45:53.137930Z",
     "start_time": "2024-11-17T17:45:53.134392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]"
   ],
   "id": "3e43077ea2a1df96",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "stemming and removing stopwords and letters which occur more than 2 times consecutive",
   "id": "687fbbbe7ea134a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T18:25:15.967408Z",
     "start_time": "2024-11-17T18:25:15.962236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_text(text):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    reduced_tokens = []\n",
    "    wrapper = [False]\n",
    "    def replace(hit):\n",
    "        if not wrapper[0]:\n",
    "            wrapper[0] = True\n",
    "            return hit[0]\n",
    "        else:\n",
    "            return \"\"\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            reduced_tokens.append(re.sub(r\"(.)(?=\\1+)\", replace, word))\n",
    "        wrapper[0] = False\n",
    "    return reduced_tokens"
   ],
   "id": "466a59c9681778e7",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:45:57.518200Z",
     "start_time": "2024-11-17T17:45:57.514969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stem_text(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ],
   "id": "a86854822bd1190a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T17:45:58.868410Z",
     "start_time": "2024-11-17T17:45:58.865337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize_text(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ],
   "id": "260f79a164640967",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "preprocessing",
   "id": "38da764f58cacf7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T18:31:21.201604Z",
     "start_time": "2024-11-17T18:25:25.043511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['mapped_sentiment'] = df.sentiment.apply(decode_sentiment)\n",
    "df['tokenized_text'] = df.text.apply(tokenize_text)\n",
    "df['stemmed_text'] = df.tokenized_text.apply(stem_text)\n",
    "df['lemmatized_text'] = df.tokenized_text.apply(lemmatize_text)\n"
   ],
   "id": "74bf3aad3a2cc006",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T18:31:33.324655Z",
     "start_time": "2024-11-17T18:31:32.660648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.head(5))\n",
    "df.info()"
   ],
   "id": "3f69b457da83ea26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         ids                      date      flag             user  \\\n",
      "0          0  1467810369 2009-04-06 23:12:45-07:00  NO_QUERY  _TheSpecialOne_   \n",
      "1          0  1467810672 2009-04-06 23:12:49-07:00  NO_QUERY    scotthamilton   \n",
      "2          0  1467810917 2009-04-06 23:12:53-07:00  NO_QUERY         mattycus   \n",
      "3          0  1467811184 2009-04-06 23:12:57-07:00  NO_QUERY          ElleCTF   \n",
      "4          0  1467811193 2009-04-06 23:12:57-07:00  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text mapped_sentiment  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...         NEGATIVE   \n",
      "1  is upset that he can't update his Facebook by ...         NEGATIVE   \n",
      "2  @Kenichan I dived many times for the ball. Man...         NEGATIVE   \n",
      "3    my whole body feels itchy and like its on fire          NEGATIVE   \n",
      "4  @nationwideclass no, it's not behaving at all....         NEGATIVE   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [aww, bummer, shoulda, got, david, carr, third...   \n",
      "1  [upset, update, facebook, texting, might, cry,...   \n",
      "2  [dived, many, times, ball, managed, save, 50, ...   \n",
      "3            [whole, body, feels, itchy, like, fire]   \n",
      "4                               [behaving, mad, see]   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0  [aww, bummer, shoulda, got, david, carr, third...   \n",
      "1  [upset, updat, facebook, text, might, cri, res...   \n",
      "2  [dive, mani, time, ball, manag, save, 50, rest...   \n",
      "3             [whole, bodi, feel, itchi, like, fire]   \n",
      "4                                  [behav, mad, see]   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [aww, bummer, shoulda, got, david, carr, third...  \n",
      "1  [upset, update, facebook, texting, might, cry,...  \n",
      "2  [dived, many, time, ball, managed, save, 50, r...  \n",
      "3             [whole, body, feel, itchy, like, fire]  \n",
      "4                               [behaving, mad, see]  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count    Dtype                     \n",
      "---  ------            --------------    -----                     \n",
      " 0   sentiment         1600000 non-null  int64                     \n",
      " 1   ids               1600000 non-null  int64                     \n",
      " 2   date              1600000 non-null  datetime64[ns, US/Pacific]\n",
      " 3   flag              1600000 non-null  object                    \n",
      " 4   user              1600000 non-null  object                    \n",
      " 5   text              1600000 non-null  object                    \n",
      " 6   mapped_sentiment  1600000 non-null  object                    \n",
      " 7   tokenized_text    1600000 non-null  object                    \n",
      " 8   stemmed_text      1600000 non-null  object                    \n",
      " 9   lemmatized_text   1600000 non-null  object                    \n",
      "dtypes: datetime64[ns, US/Pacific](1), int64(2), object(7)\n",
      "memory usage: 122.1+ MB\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "write data to file",
   "id": "89385c0cd04c5fa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T18:34:09.109045Z",
     "start_time": "2024-11-17T18:34:04.189262Z"
    }
   },
   "cell_type": "code",
   "source": "df[['mapped_sentiment', 'lemmatized_text']].to_csv(\"train_data_prepared.csv\", encoding=\"UTF-8\", index=False)",
   "id": "5f13ae08d6e4b6c4",
   "outputs": [],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data pre-processing",
   "id": "579904f852a65b0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "imports",
   "id": "3ead0d157c01359c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-20T17:13:32.138698Z",
     "start_time": "2024-11-20T17:13:32.111761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import pytz\n",
    "import dateutil.parser\n",
    "import urllib.request\n",
    "import zipfile\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants",
   "id": "e8ff3557b014606b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:13:34.568390Z",
     "start_time": "2024-11-20T17:13:34.561114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATASET\n",
    "DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING_ISO = \"ISO-8859-1\"\n",
    "DATASET_ENCODING_UTF = \"UTF-8\"\n",
    "\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "\n",
    "# TIMEZONE\n",
    "TZINFOS = { 'PDT': pytz.timezone('US/Pacific')}\n",
    "\n",
    "# FILE\n",
    "URL = 'https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
    "TEST_FILE_NAME = \"testdata.manual.2009.06.14.csv\"\n",
    "TRAIN_FILE_NAME = \"training.1600000.processed.noemoticon.csv\""
   ],
   "id": "95c5bb9b5e368fb9",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load file",
   "id": "dba55f5a42300382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:13:47.447927Z",
     "start_time": "2024-11-20T17:13:36.472039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filehandle, _ = urllib.request.urlretrieve(URL)\n",
    "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
    "train_file = zip_file_object.open(TRAIN_FILE_NAME)\n",
    "test_file = zip_file_object.open(TEST_FILE_NAME)\n",
    "f = open(TRAIN_FILE_NAME, \"wb\")\n",
    "f.write(train_file.read())\n",
    "f = open(TEST_FILE_NAME, \"wb\")\n",
    "f.write(test_file.read())"
   ],
   "id": "bf21bd366cae5f68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74326"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load Dataframe",
   "id": "13aea151acc64614"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:16:40.971921Z",
     "start_time": "2024-11-20T17:14:05.214424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(TRAIN_FILE_NAME, names=DATASET_COLUMNS, encoding=DATASET_ENCODING_ISO, header=None, converters={'date': lambda date: dateutil.parser.parse(date, tzinfos=TZINFOS)})\n",
    "df_test = pd.read_csv(TEST_FILE_NAME, names=DATASET_COLUMNS, encoding=DATASET_ENCODING_ISO, header=None, converters={'date': lambda date: dateutil.parser.parse(date, tzinfos=TZINFOS)})"
   ],
   "id": "f1be50dd5d3062fd",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "print infos",
   "id": "d726aea2a1699cb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:17:07.153614Z",
     "start_time": "2024-11-20T17:17:06.949427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.head(5))\n",
    "print(df.dtypes)\n",
    "print(df.info())    "
   ],
   "id": "98dd0fde223f6f46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         ids                      date      flag             user  \\\n",
      "0          0  1467810369 2009-04-06 23:12:45-07:00  NO_QUERY  _TheSpecialOne_   \n",
      "1          0  1467810672 2009-04-06 23:12:49-07:00  NO_QUERY    scotthamilton   \n",
      "2          0  1467810917 2009-04-06 23:12:53-07:00  NO_QUERY         mattycus   \n",
      "3          0  1467811184 2009-04-06 23:12:57-07:00  NO_QUERY          ElleCTF   \n",
      "4          0  1467811193 2009-04-06 23:12:57-07:00  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n",
      "sentiment                         int64\n",
      "ids                               int64\n",
      "date         datetime64[ns, US/Pacific]\n",
      "flag                             object\n",
      "user                             object\n",
      "text                             object\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype                     \n",
      "---  ------     --------------    -----                     \n",
      " 0   sentiment  1600000 non-null  int64                     \n",
      " 1   ids        1600000 non-null  int64                     \n",
      " 2   date       1600000 non-null  datetime64[ns, US/Pacific]\n",
      " 3   flag       1600000 non-null  object                    \n",
      " 4   user       1600000 non-null  object                    \n",
      " 5   text       1600000 non-null  object                    \n",
      "dtypes: datetime64[ns, US/Pacific](1), int64(2), object(3)\n",
      "memory usage: 73.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load stopwords",
   "id": "30401fddf4a668eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:17:08.848328Z",
     "start_time": "2024-11-20T17:17:08.838069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_test.head(5))\n",
    "print(df_test.dtypes)\n",
    "print(df_test.info())"
   ],
   "id": "4861760265a73fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment  ids                      date     flag      user  \\\n",
      "0          4    3 2009-05-11 03:17:40+00:00  kindle2    tpryan   \n",
      "1          4    4 2009-05-11 03:18:03+00:00  kindle2    vcu451   \n",
      "2          4    5 2009-05-11 03:18:54+00:00  kindle2    chadfu   \n",
      "3          4    6 2009-05-11 03:19:04+00:00  kindle2     SIX15   \n",
      "4          4    7 2009-05-11 03:21:41+00:00  kindle2  yamarama   \n",
      "\n",
      "                                                text  \n",
      "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
      "1  Reading my kindle2...  Love it... Lee childs i...  \n",
      "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
      "3  @kenburbary You'll love your Kindle2. I've had...  \n",
      "4  @mikefish  Fair enough. But i have the Kindle2...  \n",
      "sentiment                      int64\n",
      "ids                            int64\n",
      "date         datetime64[ns, tzutc()]\n",
      "flag                          object\n",
      "user                          object\n",
      "text                          object\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype                  \n",
      "---  ------     --------------  -----                  \n",
      " 0   sentiment  498 non-null    int64                  \n",
      " 1   ids        498 non-null    int64                  \n",
      " 2   date       498 non-null    datetime64[ns, tzutc()]\n",
      " 3   flag       498 non-null    object                 \n",
      " 4   user       498 non-null    object                 \n",
      " 5   text       498 non-null    object                 \n",
      "dtypes: datetime64[ns, tzutc()](1), int64(2), object(3)\n",
      "memory usage: 23.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:17:14.338080Z",
     "start_time": "2024-11-20T17:17:13.621746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ],
   "id": "b7ba91386b7871c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "map sentiment to constants",
   "id": "8ecbda08390c15d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:17:16.656421Z",
     "start_time": "2024-11-20T17:17:16.651942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]"
   ],
   "id": "3e43077ea2a1df96",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "stemming and removing stopwords and letters which occur more than 2 times consecutive",
   "id": "687fbbbe7ea134a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:17:19.375341Z",
     "start_time": "2024-11-20T17:17:19.370744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_text(text):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    reduced_tokens = []\n",
    "    wrapper = [False]\n",
    "    def replace(hit):\n",
    "        if not wrapper[0]:\n",
    "            wrapper[0] = True\n",
    "            return hit[0]\n",
    "        else:\n",
    "            return \"\"\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            reduced_tokens.append(re.sub(r\"(.)(?=\\1+)\", replace, word))\n",
    "        wrapper[0] = False\n",
    "    return reduced_tokens"
   ],
   "id": "466a59c9681778e7",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T18:50:11.479239Z",
     "start_time": "2024-11-20T18:50:11.455442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stem_text(tokens):\n",
    "    return ' '.join([stemmer.stem(word) for word in tokens])"
   ],
   "id": "a86854822bd1190a",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T18:50:12.633504Z",
     "start_time": "2024-11-20T18:50:12.629844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize_text(tokens):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])"
   ],
   "id": "260f79a164640967",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "preprocessing",
   "id": "38da764f58cacf7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T18:56:03.094017Z",
     "start_time": "2024-11-20T18:50:14.788427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['mapped_sentiment'] = df.sentiment.apply(decode_sentiment)\n",
    "df['tokenized_text'] = df.text.apply(tokenize_text)\n",
    "df['stemmed_text'] = df.tokenized_text.apply(stem_text)\n",
    "df['lemmatized_text'] = df.tokenized_text.apply(lemmatize_text)\n"
   ],
   "id": "74bf3aad3a2cc006",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T18:56:08.543786Z",
     "start_time": "2024-11-20T18:56:08.413680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test['mapped_sentiment'] = df_test.sentiment.apply(decode_sentiment)\n",
    "df_test['tokenized_text'] = df_test.text.apply(tokenize_text)\n",
    "df_test['stemmed_text'] = df_test.tokenized_text.apply(stem_text)\n",
    "df_test['lemmatized_text'] = df_test.tokenized_text.apply(lemmatize_text)"
   ],
   "id": "5c71c30c5e5fc687",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T18:56:13.351373Z",
     "start_time": "2024-11-20T18:56:12.632955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.head(5))\n",
    "df.info()"
   ],
   "id": "3f69b457da83ea26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         ids                      date      flag             user  \\\n",
      "0          0  1467810369 2009-04-06 23:12:45-07:00  NO_QUERY  _TheSpecialOne_   \n",
      "1          0  1467810672 2009-04-06 23:12:49-07:00  NO_QUERY    scotthamilton   \n",
      "2          0  1467810917 2009-04-06 23:12:53-07:00  NO_QUERY         mattycus   \n",
      "3          0  1467811184 2009-04-06 23:12:57-07:00  NO_QUERY          ElleCTF   \n",
      "4          0  1467811193 2009-04-06 23:12:57-07:00  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text mapped_sentiment  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...         NEGATIVE   \n",
      "1  is upset that he can't update his Facebook by ...         NEGATIVE   \n",
      "2  @Kenichan I dived many times for the ball. Man...         NEGATIVE   \n",
      "3    my whole body feels itchy and like its on fire          NEGATIVE   \n",
      "4  @nationwideclass no, it's not behaving at all....         NEGATIVE   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [aww, bummer, shoulda, got, david, carr, third...   \n",
      "1  [upset, update, facebook, texting, might, cry,...   \n",
      "2  [dived, many, times, ball, managed, save, 50, ...   \n",
      "3            [whole, body, feels, itchy, like, fire]   \n",
      "4                               [behaving, mad, see]   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0        aww bummer shoulda got david carr third day   \n",
      "1  upset updat facebook text might cri result sch...   \n",
      "2    dive mani time ball manag save 50 rest go bound   \n",
      "3                    whole bodi feel itchi like fire   \n",
      "4                                      behav mad see   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0        aww bummer shoulda got david carr third day  \n",
      "1  upset update facebook texting might cry result...  \n",
      "2  dived many time ball managed save 50 rest go b...  \n",
      "3                    whole body feel itchy like fire  \n",
      "4                                   behaving mad see  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count    Dtype                     \n",
      "---  ------            --------------    -----                     \n",
      " 0   sentiment         1600000 non-null  int64                     \n",
      " 1   ids               1600000 non-null  int64                     \n",
      " 2   date              1600000 non-null  datetime64[ns, US/Pacific]\n",
      " 3   flag              1600000 non-null  object                    \n",
      " 4   user              1600000 non-null  object                    \n",
      " 5   text              1600000 non-null  object                    \n",
      " 6   mapped_sentiment  1600000 non-null  object                    \n",
      " 7   tokenized_text    1600000 non-null  object                    \n",
      " 8   stemmed_text      1600000 non-null  object                    \n",
      " 9   lemmatized_text   1600000 non-null  object                    \n",
      "dtypes: datetime64[ns, US/Pacific](1), int64(2), object(7)\n",
      "memory usage: 122.1+ MB\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "write data to file",
   "id": "89385c0cd04c5fa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T18:56:18.904661Z",
     "start_time": "2024-11-20T18:56:18.892367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_test.head(5))\n",
    "df_test.info()"
   ],
   "id": "6acab6c3e972a77b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment  ids                      date     flag      user  \\\n",
      "0          4    3 2009-05-11 03:17:40+00:00  kindle2    tpryan   \n",
      "1          4    4 2009-05-11 03:18:03+00:00  kindle2    vcu451   \n",
      "2          4    5 2009-05-11 03:18:54+00:00  kindle2    chadfu   \n",
      "3          4    6 2009-05-11 03:19:04+00:00  kindle2     SIX15   \n",
      "4          4    7 2009-05-11 03:21:41+00:00  kindle2  yamarama   \n",
      "\n",
      "                                                text mapped_sentiment  \\\n",
      "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...         POSITIVE   \n",
      "1  Reading my kindle2...  Love it... Lee childs i...         POSITIVE   \n",
      "2  Ok, first assesment of the #kindle2 ...it fuck...         POSITIVE   \n",
      "3  @kenburbary You'll love your Kindle2. I've had...         POSITIVE   \n",
      "4  @mikefish  Fair enough. But i have the Kindle2...         POSITIVE   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0    [loove, kindle2, dx, cool, 2, fantastic, right]   \n",
      "1  [reading, kindle2, love, lee, childs, good, read]   \n",
      "2    [ok, first, assesment, kindle2, fucking, rocks]   \n",
      "3  [love, kindle2, mine, months, never, looked, b...   \n",
      "4            [fair, enough, kindle2, think, perfect]   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0               loov kindle2 dx cool 2 fantast right   \n",
      "1              read kindle2 love lee child good read   \n",
      "2                   ok first asses kindle2 fuck rock   \n",
      "3  love kindle2 mine month never look back new bi...   \n",
      "4                  fair enough kindle2 think perfect   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0            loove kindle2 dx cool 2 fantastic right  \n",
      "1           reading kindle2 love lee child good read  \n",
      "2            ok first assesment kindle2 fucking rock  \n",
      "3  love kindle2 mine month never looked back new ...  \n",
      "4                  fair enough kindle2 think perfect  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype                  \n",
      "---  ------            --------------  -----                  \n",
      " 0   sentiment         498 non-null    int64                  \n",
      " 1   ids               498 non-null    int64                  \n",
      " 2   date              498 non-null    datetime64[ns, tzutc()]\n",
      " 3   flag              498 non-null    object                 \n",
      " 4   user              498 non-null    object                 \n",
      " 5   text              498 non-null    object                 \n",
      " 6   mapped_sentiment  498 non-null    object                 \n",
      " 7   tokenized_text    498 non-null    object                 \n",
      " 8   stemmed_text      498 non-null    object                 \n",
      " 9   lemmatized_text   498 non-null    object                 \n",
      "dtypes: datetime64[ns, tzutc()](1), int64(2), object(7)\n",
      "memory usage: 39.0+ KB\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T19:04:52.633764Z",
     "start_time": "2024-11-20T19:04:26.729099Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv(\"train_data_prepared.csv\", encoding=DATASET_ENCODING_UTF, index=False)",
   "id": "5f13ae08d6e4b6c4",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T19:04:55.576341Z",
     "start_time": "2024-11-20T19:04:55.555802Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.to_csv(\"test_data_prepared.csv\", encoding=DATASET_ENCODING_UTF, index=False)",
   "id": "c2ab73ae27052a5",
   "outputs": [],
   "execution_count": 78
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

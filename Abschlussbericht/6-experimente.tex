\section{Experimente}
% Wie ist der Experimentaufbau, welche Evaluationsmetriken betrachten Sie?
% Beschreibung und Interpretation der Ergebnisse

\subsection{Experimentaufbau}

Für die Experimente wurden unterschiedliche Schritte in Abhängigkeit der Verfahren durchgeführt.

\subsubsection{Klassische Ansätze}\label{sec:klassische-ansaetze}

Für die klassischen Verfahren wurde neben der Genauigkeit der Klassifikation für unterschiedliche Modelle auch analysiert, inwiefern Vorverarbeitungsschritte und Tokeni\-sie\-rungs\-verfahren die Genauigkeit der Klassifikation beeinflussen.

\paragraph{Datenvorverarbeitung}
Im Rahmen der Datenvorverarbeitung wurden die verwendeten Stoppwörter und Normalisierungsverfahren variiert.

Nach \cite[S.27]{manning2009introduction} werden unter dem Begriff Stoppwörter\footnote{Beispiele sind \textit{und} sowie \textit{oder}.} Wörter verstanden, die einen geringen Informationsgehalt haben und deshalb aus Texten entfernt werden.
Es wurden drei verschiedene Verfahren zur Behandlung von Stoppwörtern verwendet: Beibehaltung aller Stoppwörter, Verwendung der Standard-\gls{nltk} Stoppwortliste und Verwendung einer eigenen Stoppwortliste zur Entfernung spezifischer Stoppwörter.

Normalisierungsverfahren dienen dazu, die Worte oder Token in Texten zu vereinheitlichen \cite[S.28]{manning2009introduction}.
Es wurden drei verschiedene Verfahren zur Normalisierung der Worte verwendet: keine Normalisierung, Lemmatisierung (mit \textit{WordNet Lemmatizer}) und Stemming (mit \textit{Porter Stemmer}).

Das Training der Modelle wurde auf Basis der vorverarbeiteten Daten durchgeführt.
Im Rahmen der Vorverarbeitung der Daten wurden die Tweets bereinigt, die einzelnen Token normalisiert und abschließend die Stoppwörter entfernt.
In Abschnitt \ref{subsec:appendix-data-preparation} im Appendix ist der Algorithmus detaillierter beschrieben (s. Algorithmus~\ref{alg:data-preparation}) und ein Verarbeitungsbeispiel gegeben.

\paragraph{Training und Evaluation}
Auf Basis der vorverarbeiteten Daten wurden die Modelle trainiert und evaluiert.

Die Texte der vorverarbeiteten Tweets werden mittels Vektorisierungsverfahren in numerische Repräsentationen bzw. Vektoren transformiert.
Es wurden zwei Vekto\-risierungs\-verfahren verwendet: die \gls{tfidf}-Vek\-to\-ri\-sie\-rung und eine \textit{Hash}-basierte Vektorisierung\footnote{Für beide Verfahren wurden die Implementierungen der \textit{scikit-learn} Bibliothek verwendet.}.
\gls{tfidf} Vektorisierung ist ein Verfahren zur Gewichtung von Termen in Texten, das die Häufigkeit der Terme in einem Dokument und die inverse Häufigkeit von Dokumenten mit diesem Termen berücksichtigt \cite[S. 119]{manning2009introduction}.
Die \textit{Hash}-basierte Vektorisierung ist ein Verfahren, das die Wörter in einem Dokument in numerische Werte umwandelt, indem es eine Hash-Funktion verwendet, um die Wörter in einen Vektor (mit fester Länge) zu kodieren\footnote{https://scikit-learn.org/stable/api/sklearn.feature\_extraction.html}.

Für die verwendeten Vektorisierungsverfahren wurden unterschiedliche Konfigurationen von n-Grammen verwendet.
n-Gramme bezeichnen nach \cite[S.33]{jm3}, die Sequenz von $n$ aufeinanderfolgenden Wörtern.
Für die Vektorisierungsverfahren wurden Instanzen mit Kombinationen von 1-Grammen, 2-Grammen und 3-Grammen\footnote{Diese n-Gramme werden auch als Uni-, Bi- und Trigramme bezeichnet.} verwendet.

Während des Trainings werden die Modelle auf den Trainingsdaten trainiert und auf den Validierungsdaten evaluiert.
Die Genauigkeit der Modelle wird abschließend auf den Testdaten evaluiert.
Algorithmus~\ref{alg:model-training} beschreibt die Schritte für das Training und die Evaluation der Modelle.

\subsubsection{Deep Learning Ansätze} \label{sec:deep-learning-ansaetze}

Für die \gls{dl} Ansätze, bei denen \gls{bert}-basierte Modelle verwendet wurden, wurden die Standard-Tokenizer der \textit{Hugging Face} Modelle verwendet, so dass keine weiteren Vorverarbeitungsschritte oder Vektorisierungsverfahren durchgeführt wurden.

\paragraph{\textit{Finetuning} der \gls{bert}-Modelle}

Die \gls{bert}-basierten Modelle \textit{twitter-roberta\hyp{}base\hyp{}sentiment} und \textit{distilbert-base-uncased} wurden über die \textit{Transfomers} Python-Bibliothek von \textit{Hugging Face} trainiert und ausgeführt.
Mit der Bibliothek lassen sich bereits initialisierte Modelle und passende Tokenisierungs-Funktionen laden.

Für das \textit{Finetuning} wurde die Standardkonfiguration der Bibliothek verwendet.
Variiert wurden die Datensatzgröße und die Lernrate.
Die Datensatzgröße bezieht sich auf die Anzahl der Tweets, die für das \textit{Finetuning} verwendet wurden.
Die Parameterkombinationen für die Lernrate und die Datensatzgröße sind in Tabelle \ref{tab:dl-params} aufgeführt und orientieren sich an dem Vorgehen von Barbieri et al. \cite{barbieri2020tweeteval}.
\begin{table}[H]
    \begin{tabular}{ll}
        \toprule
        Parameter       & Werte                                                   \\
        \midrule
        Datensatzgröße  & 2500, 5000, 7500, 10000, 15000, 20000                   \\
        Lernrate        & $1e^{-4}$, $5e^{-5}$ , $1e^{-5}$, $5e^{-6}$, $1e^{-6}$  \\
        \bottomrule
    \end{tabular}
    \caption{Parameter für das \textit{Finetuning} der \gls{bert}-Modelle}
    \label{tab:dl-params}
\end{table}

\paragraph{Verwendung der \textit{Deepseek}-Modelle}

Für den eigenen Ansatz haben wir zuerst versucht, die von \textit{Deepseek}-R1 destillierten Modelle durch \textit{Finetuning} auf den Datensatz zu trainieren.
Dafür wurde dem Modell zwei zusätzliche voll-vernetzte Schichten angefügt.
Diese Architektur entspricht dem Aufbaue des \textit{twitter-roberta-base-sentiment} Modells für die Sentiment-Analyse.
Die erste voll-vernetzte Schicht hatte als Ein- und Ausgabe die Dimensionen der letzten Schicht des Modells.
Die zweite voll-vernetzte Schicht hatte als Eingabe die Ausgabe der ersten voll-vernetzten Schicht und als Ausgabe die Anzahl der Klassen.

Die Durchführung des \textit{Finetunings} war für das kleinste destillierte \textit{Deepseek}-R1 Modell mit 1,5 Milliarden Parametern noch möglich.
Ab dem nächstgrößerem Modell (\textit{Deepseek-R1:7B}) wurden die Hardware-Anforderungen zu groß\footnote{Vergleich. mit \gls{sgd} Optimierung: 7Mrd. Parameter * 2 Byte je Gewicht * 2 Byte je Grandient = 26GB}.

Als weiteren Ansatz verwendeten wir die Deepseek-R1-Modelle nur in der Ausführung und ließen das Sentiment per Prompt klassifizieren.
Die Modelle wurden mit Ollama ausgeführt und per Pythonskript angefragt.
Die Anfragen wurden mit und ohne Query-Ausdruck (siehe Abschnitt \ref{subsec:testdata}) durchgeführt.\\
Die Anfragen mit Query-Ausdruck hatten folgende Struktur (wobie die Platzhalter $Query\-Term$ und $Tweet$ durch die entsprechenden Werte ersetzt wurden):
\begin{quote}
    Tweet sentiment? Sentiment Topic: $QueryTerm$\\
    Answer with positive or negative. Provide reasoning in JSON.\\
    Tweet: $Tweet$
\end{quote}

\subsection{Modell-Parameter und Evaluationsmetriken}

Für die ausgewählten klassischen Modelle wurden die Standard-Parameterwerte von \textit{scikit-learn} verwendet.
Für die \gls{bert}-basierten Modelle und die \textit{Deepseek}-Modelle wurden die veröffentlichten Modelle verwendet bzw. auf diesen im Rahmen des \textit{Finetunings} aufgesetzt.

Nach~\cite{wankhade2022survey} werden für die Evaluierung von Klassifikationsmodellen vor allem das \textit{Genauig\-keits\-maß}, die \textit{Präzision} oder das \textit{F1-Maß} verwendet.
Die Klassenverteilung für die positive und negative Klasse der Trainingsdaten ist ausgeglichen und die Verteilung der Testdaten ist ebenfalls relativ ausgeglichen (5 Tweets mehr für die positive Klasse).
Weil die Klassenverteilung für die Trainingsdaten ausgeglichen und für den Testdatensatz lediglich ein Unterschied von 4 Tweets zwischen der pos
Aufgrund der einfachen Interpretierbarkeit wurde deshalb das \textit{Genauigkeitsmaß} als Evaluationsmetrik verwendet.

\subsection{Ergebnisse}

Eine Übersicht über die maximalen Genauigkeiten je Modell und Ansatz ist im Anhang in Diagramm~\ref{fig:results} dargestellt.

\subsubsection{Klassische Ansätze}

\begin{table}
    \begin{tabular}{cccccc}
        \toprule
        Modell & Normalisierung & Stoppwortliste   & Anz. Merkmale & n-Gramme & Genauigkeit \\
        \midrule
        LR  & Porter  & eig. Liste & 250.000 & (1,2) & 0.859 \\
        SVM & Porter  & -          & maximal & (1,3) & 0.858 \\
        SVM & Porter  & eig. Liste & 50.000  & (1,2) & 0.858 \\
        LR  & WordNet & -          & maximal & (1,3) & 0.858 \\
        SVM & WordNet & -          & maximal & (1,3) & 0.858 \\
        LR  & Porter  & eig. Liste & 250.000 & (1,3) & 0.856 \\
        SVM & Porter  & -          & maximal & (1,2) & 0.855 \\
        LR  & -       & nltk-Liste & maximal & (1,3) & 0.853 \\
        NB  & -       & -          & maximal & (1,2) & 0.852 \\
        SVM & -       & -          & maximal & (1,3) & 0.852 \\
        \bottomrule
    \end{tabular}
    \caption{
        Top 10 Modelle nach Testgenauigkeit (Mittelwerte von drei Ausführungen).
        Die Modelle \textit{LR}, \textit{SVM} und \textit{NB} bezeichnen die Modelle \textit{Logistische Regression}, \textit{Support Vector Machine} und \textit{Naiver Bayes Klassifikator}.
        Die Bezeichner (1, k) in der Spalte \textit{n-Gramme} geben an, dass n-Gramme mit $n\in\lbrace1,\cdots,k\rbrace$ verwendet wurden.
    }
    \label{tab:top-10-models}
\end{table}

In Tabelle~\ref{tab:top-10-models} sind die Top 10 Modelle nach Testgenauigkeit aufgeführt.
Die maximal erzielte Genauigkeit beträgt $0,859$.

\paragraph{Sensitivität Modell}
In Tabelle~\ref{tab:stats-per-model} sind die Statistiken der Test Genauigkeit für die Modelle \textit{LinearSVC}, \textit{LogisticRegression} und \textit{NaiveBayes} über alle Parameter-Kombinationen aufgeführt.

Die Logistische Regressions-Modelle erzielen im Mittel die höchsten Genauigkeiten über alle Parameter-Kombinationen.
Weiterhin ist die Standardabweichung der Genauigkeiten für die Logistische Regression mit $0,022$ am niedrigsten.

Modelle auf Basis der \gls{svm} erzielen im Mittel die zweithöchsten Genauigkeiten, wobei Mittelwert und Median $~1\%$ niedriger liegen.

Die Naive Bayes Modelle erzielen im Mittel die niedrigsten Genauigkeiten, wobei die Standardabweichung mit $0,049$ am höchsten ist.
\begin{table}
    \center
    \begin{tabular}{lccccc}
        \toprule
        & \multicolumn{5}{c}{Test Genauigkeit} \\
        Modell             & Mittelwert & Median & Std.-Abweichung & Minimum & Maximum \\
        \midrule
        LR                 & 0.819      & 0.822  & 0.022           & 0.727   & 0.861 \\
        SVM                & 0.809      & 0.813  & 0.024           & 0.730   & 0.858 \\
        NB                 & 0.778      & 0.784  & 0.049           & 0.685   & 0.852 \\
        \bottomrule
    \end{tabular}
    \caption{Statistiken der Test Genauigkeit über alle Parameter-Kombinationen für die Modelle \textit{SVM}, Logistische Regression und Naiver Bayes, die durch die Kürzel LR, SVM und NB dargestellt sind.}
    \label{tab:stats-per-model}
\end{table}

\paragraph{Sensitivität Umgang mit Stoppwörtern}

Über alle Parameter-Kombinationen ist die Genauigkeit im Mittel für die Datensätze mit Entfernung der Stoppwörter höher als für die Modelle ohne Entfernung der Stoppwörter.
Weiterhin ist die Genauigkeit mit der eigens definierten Stoppwortliste höher als mit der \gls{nltk}-Liste.

\paragraph{Sensitivität Normalisierungsverfahren}

Für die unterschiedlichen Normalisierungsverfahren ergeben sich keine signifikanten Unterschiede in der Genauigkeit der Modelle.

\paragraph{Sensitivität Vektorisierungsverfahren und n-Gramme}

Für alle Modelle ist die Genauigkeit höher, wenn das \gls{tfidf}-Vektorisierungsverfahren verwendet wird.

Für alle Modelle steigt die Genauigkeit mit der Anzahl an berücksichtigten Merkmalen und wenn mehr als nur Unigramme berücksichtigt werden.
Die Verwendung von Uni- und Bigrammen führt führt im Mittel zu den höchsten Genauigkeiten.

\subsubsection{Deep Learning Ansätze}

\paragraph{Finetuning der \gls{bert}-Modelle}
Mit dem auf aktuelleren Twitter-Daten trainierten Modell \textit{twitter-roberta-base-sentiment} wurden Genauigkeiten von 0,83 auf dem Testdatensatz erzielt.
Durch \textit{Finetuning} wurden Genauigkeiten von 0,922 für das Modell \textit{twitter-roberta-base-sentiment} und 0,849 für das Modell \textit{distilbert-base-uncased} erzielt.

Kleinere Lernraten führen zu höheren Genauigkeiten für das Modell \textit{twitter-roberta-base-sentiment}, während für das Modell \textit{distilbert-base-uncased} höhere Lernraten zu besseren Ergebnissen führen.
Dies ist darauf zurückzuführen, dass das Modell \textit{twitter-roberta-base-sentiment} bereits auf Twitter-Daten trainiert wurde und das \textit{distilbert-base-uncased} Modell lediglich auf einem allgemeinen Korpus.

\paragraph{Verwendung der \textit{Deepseek}-Modelle}
Das \textit{Finetuning} des kleinsten \textit{Deepseek}-Modells lieferte Genauigkeiten von bis zu 0,866.

Für die Verwendung der \textit{Deepseek}-Modelle ohne \textit{Finetuning} mittels direkter Anfragen wurden Genauigkeiten bis zu 0,977 erzielt.
Hier gilt, dass die Genauigkeit steigt je mehr Parameter das destillierte Modell hat.
Weiterhin ist die Genauigkeit höher, wenn die Anfragen mit einem Query-Term durchgeführt werden.
Die Ergebnisse sind in Tabelle~\ref{tab:deepseek-results} zusammengefasst.
\begin{table}
    \begin{tabular}{cccccc}
        \toprule
        Modell           & Genauigkeit & Verwendung Query-Ausdruck\\
        \midrule
        DeepSeek-r1:70B  & 0.977       & Ja \\
        DeepSeek-r1:32B  & 0.966       & Ja \\
        DeepSeek-r1:8B   & 0.955       & Ja \\
        DeepSeek-r1:1.5B & 0.883       & Ja \\
        DeepSeek-r1:70B  & 0.930       & Nein \\
        DeepSeek-r1:32B  & 0.927       & Nein \\
        DeepSeek-r1:8B   & 0.916       & Nein \\
        DeepSeek-r1:1.5B & 0.824       & Nein \\
        \bottomrule
    \end{tabular}
    \caption{Genauigkeit bei Verwendung der \textit{Deepseek}-Modelle ohne \textit{Finetuning}}
    \label{tab:deepseek-results}
\end{table}

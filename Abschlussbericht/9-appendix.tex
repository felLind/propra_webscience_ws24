\section{Algorithmen}

\subsection{Datenaufbereitung}\label{subsec:appendix-data-preparation}

\textit{Hinweis}: Die Datensätze enthalten weitere Daten je Tweet, wie beispielsweise die Sentiment Klasse.
Der Umgang mit diesen Daten wurde hier aus Gründen der Übersichtlichkeit nicht aufgeführt.

Für die unterschiedlichen Parameter-Ausprägungen für die Normalisierungsverfahren und Stoppwortlisten wurden separate Datensätze erstellt.

\subsubsection{Datenaufbereitung - Algorithmus}
Die Datensätze wurden gemäß des folgenden Algorithmus erzeugt.
\begin{algorithm}
    \caption{Datenaufbereitung}
    \begin{algorithmic}[1]
        \Procedure{DatasetPreparation}{$dataset, normalizerFunction, stoppwords$}
            \Function{SanitizeTweet}{$text$}
                \State $text$ $\gets$ Entferne URLs, Nutzer, Hashtags und Sonderzeichen aus $text$
                \State \Return $text$
            \EndFunction

            \Function{NormalizeTweet}{$text, normalizerFunction, stoppwords$}
                \State $tokens$ $\gets$ Zerlege $text$ mit TweetTokenizer in Token
                \For{$i \gets 1$ \textbf{to} $|tokens|$}
                    \State $tokens$[$i$] $\gets$ $normalizerFunction$($tokens$[$i$])
                \EndFor
                \State $tokens$ $\gets$ Entferne Stoppwörter aus $tokens$ gemäß Stoppwortliste $stoppwords$
                \State $text$ $\gets$ Füge Elemente aus $tokens$ zu einem Text zusammen
                \State \Return $text$
            \EndFunction

            \For{$i \gets 1$ \textbf{to} $|dataset|$}
                \State $dataset[i]$ $\gets$ \Call{SanitizeTweet}{$dataset[i]$}
                \State $dataset[i]$ $\gets$ \Call{NormalizeTweet}{$dataset[i], normalizerFunction, stoppwords$}
            \EndFor
            \State save $dataset$
        \EndProcedure
    \end{algorithmic}
    \label{alg:data-preparation}
\end{algorithm}

\subsubsection{Datenaufbereitung - Beispiel}

Im folgenden ist ein Beispiel für die Datenaufbereitung eines Tweets dargestellt.
\begin{itemize}
    \item \textbf{Original-Tweet}: \textit{\glqq @user I love this movie! http://example.com\grqq}
    \item \textbf{Bereinigung}: \textit{\glqq I love this movie\grqq}
    \item \textbf{Tokenisierung}: \textit{\glqq I\grqq, \glqq love\grqq, \glqq this\grqq, \glqq movie\grqq}
    \item \textbf{Normalisierung}:
    \begin{itemize}
        \item \textit{Lemmatisierung}: \textit{\glqq I\grqq, \glqq love\grqq, \glqq this\grqq, \glqq movie\grqq}
        \item \textit{Stemming}: \textit{\glqq I\grqq, \glqq lov\grqq, \glqq thi\grqq, \glqq movi\grqq}
    \end{itemize}
    \item \textbf{Stoppwörter Behandlung}: Ohne Stoppwörter \glqq{}I\grqq{} und \glqq{}this\grqq{}: \textit{\glqq love\grqq, \glqq movie\grqq}
    \item \textbf{Aufbereiteter Text}: \textit{\glqq love movie\grqq}
\end{itemize}

\subsection{Training und Evaluation der Modelle}

Die Schritte für das Training und die Evaluation der Modelle sind, aufbauend auf dem durch Algorithmus \ref{alg:data-preparation} erzeugten Datensätzen wie folgt:
\begin{algorithm}
    \caption{Training und Evaluation der Modelle}
    \begin{algorithmic}[1]
        \Procedure{ModelTraining}{$dataset, vectorizer, model$}
            \State $X, y \gets$ Extrahiere Texte und Labels aus $dataset$
            \State $X \gets$ Transformiere Texte in $X$ in Vektoren mit Vektorizer $vectorizer$
            \State $X_{train}, X_{test}, y_{train}, y_{test} \gets$ Teile $X$ und $y$ in Trainings- und Testdaten
            \State $model \gets$ Trainiere $model$ auf $X_{train}$ und $y_{train}$
            \State $accuracy \gets$ Evaluiere $model$ auf $X_{test}$ und $y_{test}$
            \State \Return $accuracy$
        \EndProcedure
    \end{algorithmic}
    \label{alg:model-training}
\end{algorithm}

\section{Tabellen}

Die folgende Tabelle zeigt die einzelnen Werte für Datensatzgröße und Lernrate, welche für das \textit{Finetuning} der DL-Ansätze verwendet wurden.

\begin{table}[h]
    \center
    \begin{tabular}{lc}
        \toprule
        Parameter       & Werte                                                   \\
        \midrule
        Datensatzgröße  & 2500, 5000, 7500, 10000, 15000, 20000                   \\
        Lernrate        & $10^{-4}$, $5\cdot 10^{-5}$ , $10^{-5}$, $5\cdot 10^{-6}$, $10^{-6}$  \\
        \bottomrule
    \end{tabular}
    \caption{Parameter für das \textit{Finetuning} der \gls{bert}-Modelle}
    \label{tab:dl-params}
\end{table}

\section{Diagramme}

Im folgenden Diagramm sind für alle Modelle und Verfahren die maximalen Genauigkeiten dargestellt.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/alle-übersicht-genauigkeit-alle-modelle.png}
    \caption{Maximale Genauigkeit der Modelle und Verfahren}
    \label{fig:results}
\end{figure}

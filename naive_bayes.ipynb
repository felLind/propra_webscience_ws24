{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with naive-bayes\n",
    "\n",
    "The aim of this notebook is to try to predict the sentiment and emotion of tweets using a naive Bayes classifier. The tweets will be in text form and the possible categories for this classification task will be *positive* or *negative*.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset that we'll be using is the proven and tested dataset *sentiment140* consisting of 1.6 million tweets extracted using the twitter api. Half of the tweets are annotated with 'positive' and half of them are annotated with 'negative'. The methodology of this annotation is to detect tweets that use certain emoticons, use the corresponding emotion to categorize the tweet, and then remove the emoji from the text. \n",
    "\n",
    "The detailed approach can be found in the official paper: http://http//cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\n",
    "\n",
    "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Burak.Oezkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Burak.Oezkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Burak.Oezkan\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# machine learning\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Data\n",
    "\n",
    "Because the naive Bayes classifier doesn't take into account the surrounding context of words, it makes sense to remove as much noise from the text as possible in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('datasets/sentiment140.csv', names=['sentiment', 'id', 'date', 'query', 'user', 'text'])\n",
    "df = df.drop(['id', 'date', 'query', 'user'], axis=1)\n",
    "\n",
    "# remove usernames\n",
    "def remove_username(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('@'))\n",
    "\n",
    "# remove urls\n",
    "def remove_url(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('http') and not word.startswith('https')  and not word.startswith('www')) \n",
    "\n",
    "# remove non-alphabetic characters\n",
    "def remove_nonalphabet(text):\n",
    "    for char in text:\n",
    "        if char not in 'abcdefghijklmnopqrstuvwxyz'+' ':\n",
    "            text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# convert lowercase\n",
    "def convert_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(text):\n",
    "    return ' '.join(WordNetLemmatizer().lemmatize(word) for word in text.split())\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = convert_lowercase(text)\n",
    "    text = remove_username(text)\n",
    "    text = remove_url(text)\n",
    "    text = lemmatize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_nonalphabet(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing and save new dataset\n",
    "df_preprocessed = df.copy()\n",
    "df_preprocessed['text'] = df_preprocessed['text'].apply(preprocess)\n",
    "df_preprocessed.to_csv('datasets/sentiment140_preprocessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with naive-bayes\n",
    "\n",
    "The aim of this notebook is to try to predict the sentiment and emotion of tweets using a naive Bayes classifier. The tweets will be in text form and the possible categories for this classification task will be *positive* or *negative*.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset that we'll be using is the proven and tested dataset *sentiment140* consisting of 1.6 million tweets extracted using the twitter api. Half of the tweets are annotated with 'positive' and half of them are annotated with 'negative'. The methodology of this annotation is to detect tweets that use certain emoticons, use the corresponding emotion to categorize the tweet, and then remove the emoji from the text. \n",
    "\n",
    "The detailed approach can be found in the official paper: http://http//cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\n",
    "\n",
    "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Burak.Oezkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Burak.Oezkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Burak.Oezkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# machine learning\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Data\n",
    "\n",
    "Because the naive Bayes classifier doesn't take into account the surrounding context of words, it makes sense to remove as much noise from the text as possible in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('datasets/sentiment140.csv', names=['sentiment', 'id', 'date', 'query', 'user', 'text'])\n",
    "df = df.drop(['id', 'date', 'query', 'user'], axis=1)\n",
    "\n",
    "# remove usernames\n",
    "def remove_username(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('@'))\n",
    "\n",
    "# remove urls\n",
    "def remove_url(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('http') and not word.startswith('https')  and not word.startswith('www')) \n",
    "\n",
    "# remove non-alphabetic characters\n",
    "def remove_nonalphabet(text):\n",
    "    for char in text:\n",
    "        if char not in 'abcdefghijklmnopqrstuvwxyz'+' ':\n",
    "            text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# convert lowercase\n",
    "def convert_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(text):\n",
    "    return ' '.join(WordNetLemmatizer().lemmatize(word) for word in text.split())\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = convert_lowercase(text)\n",
    "    text = remove_username(text)\n",
    "    text = remove_url(text)\n",
    "    text = lemmatize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_nonalphabet(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing and save new dataset\n",
    "df_preprocessed = df.copy()\n",
    "df_preprocessed['text'] = df_preprocessed['text'].apply(preprocess)\n",
    "df_preprocessed.to_csv('datasets/sentiment140_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_preprocessed['text'], df_preprocessed['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfid-Vectorizer\n",
    "\n",
    "Additionally we need to vectorize our dataset, so that we can use our ML-models.\n",
    "\n",
    "We train our Vectorizer on our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=500000, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=500000, ngram_range=(1, 2))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=500000, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform our train and test sets using the vectoriser we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.77    159494\n",
      "           4       0.77      0.81      0.79    160506\n",
      "\n",
      "    accuracy                           0.78    320000\n",
      "   macro avg       0.78      0.78      0.78    320000\n",
      "weighted avg       0.78      0.78      0.78    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we are using a Bernoulli Naive Bayes model\n",
    "clf = BernoulliNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The training time for the naive Bayes classifier was extremely fast\n",
    "* The metrics may not be the best, but an accuracy of 0.78 is not bad at all\n",
    "* Especially when you consider the data we work with\n",
    "    * The annotations of the tweets were automated and classified by the use of emojis that may not correlate with the text of the tweet\n",
    "    * The classification of sentiment in positive and negative tweets doesn't model is very simplified\n",
    "    * Doesn't account for the strength of the sentiment\n",
    "    * Doesn't account for neutral tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test without preprocessing\n",
    "\n",
    "Let's look at our model without the use our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us test this without preprocessing\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "vectoriser2 = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser2.fit(X_train2)\n",
    "X_train2 = vectoriser2.transform(X_train2)\n",
    "X_test2 = vectoriser2.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80    159494\n",
      "           4       0.80      0.81      0.80    160506\n",
      "\n",
      "    accuracy                           0.80    320000\n",
      "   macro avg       0.80      0.80      0.80    320000\n",
      "weighted avg       0.80      0.80      0.80    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf2 = BernoulliNB().fit(X_train2, y_train2)\n",
    "y_pred2 = clf2.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible reasons why it's better without preprocessing\n",
    "* 1-2 ngrams are looking at more than single words, removing stopwords might be bad\n",
    "* Extensive removal of text is losing data\n",
    "\n",
    "Let us test this without removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_no_stopwords(text):\n",
    "    text = convert_lowercase(text)\n",
    "    text = remove_username(text)\n",
    "    text = remove_url(text)\n",
    "    text = lemmatize(text)\n",
    "    text = remove_nonalphabet(text)\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv('datasets/sentiment140.csv', names=['sentiment', 'id', 'date', 'query', 'user', 'text'])\n",
    "df = df.drop(['id', 'date', 'query', 'user'], axis=1)\n",
    "\n",
    "df_preprocessed2 = df.copy()\n",
    "df_preprocessed2['text'] = df_preprocessed2['text'].apply(preprocess_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79    159494\n",
      "           4       0.79      0.82      0.80    160506\n",
      "\n",
      "    accuracy                           0.80    320000\n",
      "   macro avg       0.80      0.80      0.80    320000\n",
      "weighted avg       0.80      0.80      0.80    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df_preprocessed2['text'], df_preprocessed2['sentiment'], test_size=0.2, random_state=42)\n",
    "vectoriser2 = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser2.fit(X_train2)\n",
    "X_train2 = vectoriser2.transform(X_train2)\n",
    "X_test2 = vectoriser2.transform(X_test2)\n",
    "clf2 = BernoulliNB().fit(X_train2, y_train2)\n",
    "y_pred2 = clf2.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a meaningful difference to applying no preprocessing.\n",
    "Conclusion: preprocessing for this task is probably not necessary. \n",
    "\n",
    "Note: Try Tweet-tokenizer from sklearn package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using official test data\n",
    "\n",
    "There is manually labeled test data for this dataset. This would give us a more accurate evaluation and could improve our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"datasets/testdata.csv\", names=['sentiment', 'id', 'date', 'query', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(df_test[df_test['sentiment'] == 2].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dropping all the rows that include the neutral sentiment (sentiment = 2) because there is no neutral sentiment in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test3 = df_test['text']\n",
    "y_test3 = df_test['sentiment']\n",
    "X_train3 = df['text']\n",
    "y_train3 = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser3 = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser3.fit(X_train3)\n",
    "X_train3 = vectoriser3.transform(X_train3)\n",
    "X_test3 = vectoriser3.transform(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.83       177\n",
      "           4       0.82      0.85      0.83       182\n",
      "\n",
      "    accuracy                           0.83       359\n",
      "   macro avg       0.83      0.83      0.83       359\n",
      "weighted avg       0.83      0.83      0.83       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf3 = BernoulliNB().fit(X_train3, y_train3)\n",
    "y_pred3 = clf3.predict(X_test3)\n",
    "print(classification_report(y_test3, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a noticeable improvement with the hand-labeled test data compared to splitting the test data from our training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
